# Saliency-Maps-for-interpretability

Neural networks are powerful tools, but they are black-boxes, meaning that it is difficult to provide human-understandable explanations on what they are doing. 
The field of explanaibility is concerned with finding algorithms for achieving this.

In this homework, propoused by Professor  Scardapane in its course Neural Networks for Data Science Applications 23-24, I applied the Saliency Maps technique on the CIFAR10 dataset in order to grasp some intepretation of the classification task.


![barca_a_vela](https://github.com/FedericoAlvetreti/Saliency-Maps-for-interpretability/assets/115395996/cef9c7aa-0cd5-4156-b148-90469bdf375b)
